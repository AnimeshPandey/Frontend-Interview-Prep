# üöÄ Interview Gold ‚Äì Batch #10 (Scalability & Reliability, Super Expanded)

---

## 1. Handling Traffic Spikes (1M users hitting at once)

**Problem:**

* Product launch (e.g., iPhone preorder, sneaker drop, ticket sales).
* Sudden surge ‚Üí frontend JS bundle loads fine from CDN, but API/backend crashes.
* Symptoms: blank UI, errors, long spinners ‚Üí terrible UX.

**Solution:**

* **Static-first strategy**: Serve all JS/CSS/images from CDN.
* **Server-Side Rendering (SSR) caching** or **Incremental Static Regeneration (ISR)** for pages that don‚Äôt change often.
* **Queue UI**: Instead of letting requests fail, place users in ‚Äúwaiting room‚Äù when backend is overloaded.
* **Graceful degradation**: show cached results or partial UI if API down.

**Detailed Design:**

1. **Frontend Bundles** ‚Üí deployed to CDN (immutable hashed filenames).
2. **API Load Shedding**: If traffic > X QPS, backend returns 503 + Retry-After.
3. **Frontend Behavior**:

   * If API fails, show cached data (IndexedDB/Service Worker).
   * If no cached data ‚Üí show fallback + auto-retry with exponential backoff.
4. **Waiting Room**: Controlled at CDN/edge (Cloudflare Waiting Room).

**Performance Notes:**

* 95% of load is static assets ‚Üí solved by CDN.
* True bottleneck = API.
* **Mitigation:**

  * Cache GETs at edge.
  * Collapse duplicate requests (dedup fetches on client).

**Pitfalls:**

* If API responses aren‚Äôt cacheable, every user hits backend.
* If CDN config wrong (no cache headers), origin gets slammed.

**Real-World Example:**

* Ticketmaster, Nike drops ‚Üí waiting room UI prevents meltdown.

**Follow-up Qs:**

* How would you ensure fairness? ‚Üí Randomized queue tokens.
* How do you avoid overloading DB? ‚Üí Cache at multiple layers (edge + app cache).
* What if real-time stock levels? ‚Üí Edge workers validate stock before allowing purchase.

---

## 2. CDN Strategy for Global Users

**Problem:**

* Users in India/Europe access server in US ‚Üí 200‚Äì400ms latency.
* API/asset delivery slows down, especially images and video.

**Solution:**

* Use **CDNs (Cloudflare, Akamai, Fastly)** for static content.
* Push static bundles to **edge PoPs** (Points of Presence).
* Cache API responses at edge where possible.
* Use **edge compute** (Cloudflare Workers, Vercel Edge) for lightweight personalization.

**Detailed Design:**

1. **Static Assets**:

   * Served from CDN edge, immutable caching (`max-age=31536000, immutable`).
   * Versioning via filename hashes.
2. **APIs**:

   * Cache non-sensitive data (product catalogs, trending feeds).
   * Use **stale-while-revalidate** for freshness.
3. **Images/Media**:

   * Use CDN-based transformations (resize, compress, WebP/AVIF).
   * Serve right size per device.

**Performance Notes:**

* CDN latency \~20ms vs 200ms from origin.
* Global scale requires **multi-CDN failover** (e.g., fallback from Fastly ‚Üí Akamai).

**Pitfalls:**

* Don‚Äôt cache personalized responses ‚Üí risk of data leakage.
* Cache invalidation hard ‚Üí prefer immutable assets.

**Real-World Example:**

* Netflix Open Connect ‚Üí servers inside ISPs for local video delivery.

**Follow-up Qs:**

* How do you purge CDN cache? ‚Üí API call to purge or versioned URLs.
* How to handle real-time personalized data (e.g., cart)? ‚Üí Bypass cache, fetch directly.
* Compare CDN caching vs Service Worker caching.

---

## 3. Graceful Degradation & Offline Mode

**Problem:**

* Users on 2G/3G or airplane mode ‚Üí fetches fail.
* Without fallback, app looks broken.

**Solution:**

* **Graceful degradation**: core features always work, advanced ones may break.
* **Progressive enhancement**: app designed to add features if browser/network allows.
* **Offline-first**: Service Worker caches assets + data.
* **Request queueing**: API calls stored offline, replayed later.

**Detailed Design:**

1. **Static assets** ‚Üí cached with Service Worker (App Shell model).
2. **API calls** ‚Üí stored in IndexedDB if offline.
3. **Sync** ‚Üí Background Sync API retries when network returns.
4. **UI** ‚Üí show offline banner, disable non-critical actions.

**Performance Notes:**

* UX improvement ‚Üí users can ‚Äúread cached content‚Äù offline.
* Battery-friendly: avoid aggressive retry loops.

**Pitfalls:**

* If not careful, duplicate requests when replayed. Use unique request IDs.

**Real-World Example:**

* Twitter Lite (600KB, works offline).
* Gmail caches last emails offline.

**Follow-up Qs:**

* How do you prevent data corruption during replay? ‚Üí Idempotent APIs.
* Progressive enhancement vs graceful degradation? ‚Üí Enhancement starts small and builds up, degradation starts big and cuts down.
* How to sync large offline datasets efficiently? ‚Üí Delta sync instead of full refresh.

---

## 4. Resiliency Patterns (Frontend Fallbacks)

**Problem:**

* Backend might fail or be slow (timeouts, 500 errors).
* UI must not freeze or crash ‚Üí user retention risk.

**Solution:**

* **Retry with exponential backoff**.
* **Circuit breaker pattern** ‚Üí stop hitting failing API.
* **Fallback data** ‚Üí cached data, last known good.
* **UI fallback** ‚Üí skeleton loaders, error messages.

**Detailed Design:**

```js
async function fetchWithCircuitBreaker(url) {
  if (circuit.isOpen()) return getCached(url);

  try {
    return await fetch(url);
  } catch {
    circuit.recordFailure();
    return getCached(url);
  }
}
```

* Circuit transitions:

  * **Closed**: working.
  * **Open**: failures ‚Üí short-circuit to fallback.
  * **Half-open**: try again after timeout.

**Performance Notes:**

* Protects backend from ‚Äúretry storms.‚Äù
* User sees degraded UI but not total crash.

**Pitfalls:**

* Too aggressive circuit ‚Üí false positives.
* Caching must be consistent with stale data handling.

**Real-World Example:**

* Netflix ‚Äúrows‚Äù sometimes show old cached titles if API down.

**Follow-up Qs:**

* How do you show degraded data without confusing users? ‚Üí Grey out, mark ‚Äúlast updated at‚Ä¶‚Äù.
* How to log failures without flooding logs? ‚Üí Use sampling.
* Difference between retry vs circuit breaker?

---

## 5. API Rate Limiting & Throttling

**Problem:**

* User types fast in search box ‚Üí 50 API requests/second.
* Backend overloads.

**Solution:**

* **Client-side debounce/throttle** to cut requests.
* **Server-side rate limits** (per-user, per-IP).
* **Backpressure**: respond with `429 Too Many Requests`.

**Detailed Design:**

* Client:

```js
const search = debounce(query => fetch(`/api?q=${query}`), 300);
```

* Server:

```http
HTTP/1.1 429 Too Many Requests
Retry-After: 5
```

**Performance Notes:**

* Greatly reduces wasted calls.
* Prevents backend overload.

**Pitfalls:**

* If user behind shared IP (corporate, NAT) ‚Üí false positives.

**Real-World Example:**

* GitHub API has 5000 requests/hour per token.

**Follow-up Qs:**

* Difference debounce vs throttle? ‚Üí Debounce = wait until quiet; Throttle = limit to once per interval.
* How to tell user they hit rate limit? ‚Üí ‚ÄúPlease wait‚Ä¶‚Äù or retry-after UI.
* How to design APIs for resilience? ‚Üí Idempotent operations, pagination.

---

## 6. Multi-Region Failover

**Problem:**

* Cloud provider outage in one region ‚Üí entire app down.

**Solution:**

* Deploy app in **multiple regions** (multi-AZ, multi-region).
* Use **DNS-based failover** (Route53, Cloudflare Load Balancer).
* Store data in replicated DB (eventual consistency).

**Detailed Design:**

* User connects ‚Üí global DNS sends to nearest healthy region.
* If region unhealthy ‚Üí DNS reroutes to backup region.
* Static assets ‚Üí always served via CDN globally.

**Performance Notes:**

* Improves availability (99.99%).
* Latency minimized by routing to closest region.

**Pitfalls:**

* Multi-region DB consistency is hard.
* Expensive to maintain redundant infra.

**Real-World Example:**

* Netflix Chaos Monkey kills regions to test resilience.

**Follow-up Qs:**

* How do you keep user data consistent? ‚Üí Eventual consistency (Cassandra/Dynamo).
* How to test failover? ‚Üí Chaos experiments.
* Tradeoff: CAP theorem (Consistency vs Availability).

---

## 7. Error Budgets & SLOs (Site Reliability)

**Problem:**

* Teams push features at cost of stability.
* No agreement on ‚Äúhow reliable is reliable enough?‚Äù

**Solution:**

* **SLO (Service Level Objective)**: target reliability, e.g., 99.9% uptime.
* **SLI (Service Level Indicator)**: actual measurement (e.g., error rate <0.1%).
* **Error Budget**: allowable downtime = (100% - SLO).

**Detailed Design:**

* Example: 99.9% uptime = \~40 min downtime/month.
* If budget exceeded ‚Üí freeze feature rollouts, focus on stability.

**Performance Notes:**

* Balances velocity with reliability.
* Avoids chasing ‚Äú100% uptime,‚Äù which is near-impossible.

**Pitfalls:**

* Badly defined SLIs ‚Üí meaningless SLOs.
* Teams may game metrics (only measure subset).

**Real-World Example:**

* Google SRE pioneered error budgets.

**Follow-up Qs:**

* Why not 100% uptime? ‚Üí too costly, diminishing returns.
* How do you measure frontend SLIs? ‚Üí Apdex score, Web Vitals, JS error rate.
* What happens when error budget is exceeded? ‚Üí Feature freeze.

---

## 8. Resilient Frontend Logging & Monitoring

**Problem:**

* Without telemetry ‚Üí silent failures.
* Users see errors but devs don‚Äôt know.

**Solution:**

* **Error monitoring**: Sentry, Rollbar, Datadog.
* **Perf monitoring**: Web Vitals (LCP, CLS, FID).
* **Logging with sampling**: avoid flooding servers.

**Detailed Design:**

```js
window.addEventListener("error", e => {
  sendToSentry({ message: e.message, stack: e.error.stack });
});

window.addEventListener("unhandledrejection", e => {
  sendToSentry({ message: e.reason?.toString() });
});
```

* Collect perf metrics with `PerformanceObserver`.
* Sample \~1% of sessions for detailed logging.

**Performance Notes:**

* Errors must be non-blocking (async reporting).
* Scrub sensitive data before sending.

**Pitfalls:**

* Logging PII can break compliance (GDPR).
* Too much logging = high cost.

**Real-World Example:**

* Airbnb samples logs to 1% of sessions ‚Üí balance cost vs insights.

**Follow-up Qs:**

* How do you monitor frontend performance in prod? ‚Üí Web Vitals API.
* How to debug minified JS errors? ‚Üí Upload source maps.
* How to avoid logging sensitive data? ‚Üí Use allowlist logging.

---

# üìò Key Takeaways ‚Äì Batch #10 (Expanded)

* **Traffic spikes:** CDN, ISR, waiting rooms.
* **CDN strategy:** edge caching + edge compute.
* **Offline/degradation:** Service Worker, retries, queue API calls.
* **Resilience:** retries, circuit breakers, fallback UIs.
* **Rate limiting:** debounce/throttle, 429 handling.
* **Multi-region failover:** DNS routing, eventual consistency.
* **Error budgets:** balance reliability vs features.
* **Monitoring:** Sentry + Web Vitals + sampling.

---

# üìë Quick-Reference (Batch #10)

* **Spikes:** CDN + queues + cache.
* **CDN:** immutable assets, SWR.
* **Offline:** SW + IndexedDB + background sync.
* **Resilience:** retries + circuit breakers.
* **Rate limits:** debounce vs throttle.
* **Failover:** multi-region DNS.
* **SLOs:** measurable uptime goals.
* **Monitoring:** errors + perf + sampling.