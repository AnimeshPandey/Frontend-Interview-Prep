# ðŸš€ Interview Gold â€“ Batch #17 (Frontend Reliability & Chaos Engineering Gold)

---

## 1. Feature Flags & Rollbacks

**Problem:**

* Deploying features directly = risky.
* If bug hits prod, rollback may take **30+ min redeploy** â†’ costly outage.

**Solution:**

* **Feature flags**: control features dynamically via config.
* **Instant rollbacks**: disable flag â†’ no redeploy needed.

**Detailed Design:**

* Tools: LaunchDarkly, Unleash, homemade flag service.
* Example flag:

  ```json
  { "feature_checkout_v2": { "enabled": true, "users": ["beta"] } }
  ```
* Client checks flag on render:

  ```js
  if (flags.feature_checkout_v2) return <NewCheckout />;
  return <OldCheckout />;
  ```
* Flags fetched from CDN or config service.

**Performance/Scaling Notes:**

* Rollouts can be **percentage-based** (1% â†’ 100%).
* Flags add branching â†’ slight perf cost.

**Pitfalls:**

* Too many flags â†’ â€œflag debt.â€
* Forgetting to clean old flags â†’ messy code.

**Real-world Example:**

* Facebook does **dark launches**: 1% traffic, rollback instantly if issues.

**Follow-ups:**

* How do you avoid flag debt? â†’ Regular cleanup process.
* Whatâ€™s difference between feature flag vs A/B test? â†’ Flags = control exposure, A/B = measure impact.
* How to secure flags? â†’ Signed configs, avoid tampering.

---

## 2. Canary Deployments

**Problem:**

* Deploying to all users at once â†’ huge blast radius if broken.

**Solution:**

* **Canary release**: release to small subset (1% users or 1 region).
* Monitor â†’ then roll out gradually.

**Detailed Design:**

* Progressive rollout pipeline:

  * Stage 1: 1% users.
  * Stage 2: 10%.
  * Stage 3: 50%.
  * Stage 4: 100%.
* Config in CDN or feature flag system.

**Performance/Scaling Notes:**

* Reduces risk of mass outages.
* Users may see different versions â†’ need consistency rules.

**Pitfalls:**

* Hard to debug if canary users complain but devs canâ€™t reproduce.
* Rollout stuck if monitoring unclear.

**Real-world Example:**

* Google Search: canaries new ranking algos to 0.1% queries before full rollout.

**Follow-ups:**

* Difference between canary vs blue-green deployment? â†’ Canary = gradual, blue-green = instant switch.
* How do you pick canary population? â†’ Random, or low-risk geography.
* How to ensure consistency in user experience? â†’ Sticky sessions.

---

## 3. Circuit Breakers in Frontend

**Problem:**

* Downstream API is slow/failing.
* Frontend retries aggressively â†’ worsens outage.

**Solution:**

* **Circuit breaker pattern** in frontend:

  * After N failures, â€œopen circuit.â€
  * Stop calling API, use cached/fallback data.
  * After timeout, â€œhalf-openâ€ â†’ test call.

**Detailed Design:**

```js
class CircuitBreaker {
  constructor(failLimit = 3, resetTime = 5000) {
    this.failCount = 0;
    this.open = false;
    this.resetTime = resetTime;
  }
  async call(fn) {
    if (this.open) return "fallback";
    try {
      const res = await fn();
      this.failCount = 0;
      return res;
    } catch {
      this.failCount++;
      if (this.failCount >= 3) {
        this.open = true;
        setTimeout(() => (this.open = false), this.resetTime);
      }
      return "fallback";
    }
  }
}
```

**Performance/Scaling Notes:**

* Prevents **retry storms**.
* Improves user experience with **cached data**.

**Pitfalls:**

* Over-aggressive breaker â†’ blocks even when service is recovering.
* Requires monitoring to tune thresholds.

**Real-world Example:**

* Netflix frontend â†’ shows cached movie rows if recommendation API fails.

**Follow-ups:**

* Difference between retry + circuit breaker?
* How to show fallback UI gracefully? â†’ Skeleton loaders, last-seen data.
* How to tune thresholds? â†’ Based on SLIs (e.g., 50% error rate).

---

## 4. Chaos Engineering in Frontend

**Problem:**

* Apps may handle happy-path well, but fail badly under chaos.

**Solution:**

* Inject **controlled failures** (chaos tests) into frontend.

**Detailed Design:**

* Simulate:

  * API latency (delay responses).
  * API errors (500s, 429s).
  * Network offline (use DevTools throttling).
* Chaos middleware:

  ```js
  async function fetchChaos(url) {
    if (Math.random() < 0.1) throw new Error("Chaos injected!");
    return fetch(url);
  }
  ```
* Test resilience of UI: retries, fallbacks, offline mode.

**Performance/Scaling Notes:**

* Identifies resilience gaps early.
* Avoids â€œperfect prod, broken real-world.â€

**Pitfalls:**

* Must run chaos tests in staging, not live prod (except controlled A/B).

**Real-world Example:**

* Netflix â€œChaos Monkeyâ€ kills random services; frontend must survive.

**Follow-ups:**

* How to apply chaos engineering safely in frontend?
* How to simulate flaky mobile networks? â†’ DevTools throttling, Charles Proxy.
* Why chaos improves reliability? â†’ Surfaces hidden coupling.

---

## 5. Failover Strategies (Multi-Region & CDN)

**Problem:**

* CDN or region outage â†’ users canâ€™t load app.

**Solution:**

* **Failover architecture**:

  * Static assets in multiple CDNs.
  * APIs in multiple cloud regions.
  * DNS routing for healthy regions.

**Detailed Design:**

* Multi-CDN config:

  * Primary: Cloudflare.
  * Secondary: Akamai.
  * Route53 health checks redirect on failure.

**Performance/Scaling Notes:**

* Increases cost, but ensures **high availability (99.99%)**.

**Pitfalls:**

* Data consistency issues across regions.
* DNS TTL may delay failover.

**Real-world Example:**

* Slack multi-region failover â†’ AWS us-east outage didnâ€™t kill chat.

**Follow-ups:**

* Difference between active-active vs active-passive failover?
* How to ensure data consistency? â†’ Eventual consistency, CRDTs.
* How to test failover? â†’ Chaos drills.

---

## 6. User Impact Monitoring (SLIs/SLOs)

**Problem:**

* Even with resilient systems, need to **measure user impact**.
* Engineering team may not notice degraded UX.

**Solution:**

* Define **SLIs (Service Level Indicators)** â†’ metrics like:

  * JS error rate (% sessions with uncaught errors).
  * LCP (Largest Contentful Paint).
  * API success rate.
* Define **SLOs (Objectives)** â†’ e.g., 99.9% API success rate.
* Define **Error Budget** = allowed failure % per quarter.

**Detailed Design:**

* Collect RUM metrics from browsers.
* Alert if SLO violated.

**Performance/Scaling Notes:**

* Balances feature velocity with stability.
* Prevents endless chasing of 100% uptime (too expensive).

**Pitfalls:**

* Bad SLIs = useless monitoring.
* Teams may ignore error budget policy.

**Real-world Example:**

* Google SRE pioneered SLOs + error budgets.

**Follow-ups:**

* Why not aim for 100% uptime? â†’ Diminishing returns, too costly.
* How to measure frontend SLIs? â†’ Web Vitals + error logging.
* What do you do if error budget exceeded? â†’ Freeze features, focus on reliability.

---

# ðŸ“˜ Key Takeaways â€“ Batch #17

* **Feature Flags** â†’ instant rollback, controlled rollout.
* **Canary Deploys** â†’ release gradually, monitor impact.
* **Circuit Breakers** â†’ prevent retry storms, use fallbacks.
* **Chaos Testing** â†’ inject failures to test resilience.
* **Failover** â†’ multi-region, multi-CDN for availability.
* **User Impact Monitoring** â†’ SLIs, SLOs, error budgets.

---

# ðŸ“‘ Quick-Reference (Batch #17)

* **Flags**: config-based feature control.
* **Canary**: 1% â†’ 10% â†’ 100%.
* **Breaker**: stop retries after failures.
* **Chaos**: simulate latency, errors.
* **Failover**: multi-CDN, DNS routing.
* **SLIs/SLOs**: metrics + objectives.