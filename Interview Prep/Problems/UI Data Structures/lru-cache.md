# ðŸ”Ž Problem 11: LRU Cache
* Step 1 â†’ Naive cache with eviction.
* Step 2 â†’ Add usage tracking.
* Step 3 â†’ Optimize to O(1) with **HashMap + Doubly Linked List**.
* Step 4 â†’ Add delete & clear.
* Step 5 â†’ Discuss frontend use cases & perf tradeoffs.
---

## Step 1. Interviewer starts:

*"Implement a cache with `get` and `put`. If capacity is exceeded, evict the oldest item."*

---

### âŒ Naive FIFO Cache

```js
class FIFOCache {
  constructor(limit) {
    this.limit = limit;
    this.map = new Map();
  }

  get(key) {
    return this.map.get(key);
  }

  put(key, value) {
    if (this.map.size >= this.limit) {
      // Evict first inserted
      const oldestKey = this.map.keys().next().value;
      this.map.delete(oldestKey);
    }
    this.map.set(key, value);
  }
}
```

âš ï¸ Problem: This is **FIFO (First-In-First-Out)**, not LRU.
LRU must evict the **least recently used** item, not just the oldest.

---

## Step 2. Interviewer says:

*"Good. Now track usage so that `get` marks an item as recently used."*

---

### âœ… Using `Map` (O(n) for eviction order)

```js
class LRUCache {
  constructor(limit) {
    this.limit = limit;
    this.map = new Map();
  }

  get(key) {
    if (!this.map.has(key)) return -1;
    const value = this.map.get(key);
    // Refresh usage by reinserting
    this.map.delete(key);
    this.map.set(key, value);
    return value;
  }

  put(key, value) {
    if (this.map.has(key)) this.map.delete(key);
    else if (this.map.size >= this.limit) {
      // Evict least recently used (first item in Map)
      const oldestKey = this.map.keys().next().value;
      this.map.delete(oldestKey);
    }
    this.map.set(key, value);
  }
}
```

âœ” Works, but eviction requires iterating over `Map.keys()` â†’ **O(1) in practice but not guaranteed by spec**.

---

## Step 3. Interviewer pushes:

*"Can you make both `get` and `put` **O(1)** guaranteed?"*

ðŸ‘‰ Answer: Use **HashMap + Doubly Linked List**.

---

### âœ… Optimal O(1) LRU Cache

```js
class Node {
  constructor(key, value) {
    this.key = key;
    this.value = value;
    this.prev = this.next = null;
  }
}

class LRUCache {
  constructor(limit) {
    this.limit = limit;
    this.map = new Map(); // key â†’ node
    this.head = new Node(); // dummy head
    this.tail = new Node(); // dummy tail
    this.head.next = this.tail;
    this.tail.prev = this.head;
  }

  _remove(node) {
    node.prev.next = node.next;
    node.next.prev = node.prev;
  }

  _insert(node) {
    node.next = this.head.next;
    node.prev = this.head;
    this.head.next.prev = node;
    this.head.next = node;
  }

  get(key) {
    if (!this.map.has(key)) return -1;
    const node = this.map.get(key);
    this._remove(node);
    this._insert(node); // move to front (MRU)
    return node.value;
  }

  put(key, value) {
    if (this.map.has(key)) {
      this._remove(this.map.get(key));
    } else if (this.map.size >= this.limit) {
      // Evict LRU (node before tail)
      const lru = this.tail.prev;
      this._remove(lru);
      this.map.delete(lru.key);
    }
    const newNode = new Node(key, value);
    this._insert(newNode);
    this.map.set(key, newNode);
  }
}
```

âœ” Now both `get` and `put` are **O(1)**.

---

## Step 4. Interviewer adds:

*"Cool. Now add support for `delete(key)` and `clear()`."*

---

### âœ… Extended API

```js
delete(key) {
  if (!this.map.has(key)) return false;
  const node = this.map.get(key);
  this._remove(node);
  this.map.delete(key);
  return true;
}

clear() {
  this.map.clear();
  this.head.next = this.tail;
  this.tail.prev = this.head;
}
```

---

## Step 5. Interviewer final boss:

*"Great. How would you use LRU cache in **frontend systems**? What about performance tradeoffs?"*

---

### âœ… Real-World Usage

* **Frontend Use Cases**:

  * Image caching (keep recent images in memory).
  * API response caching (GraphQL, REST).
  * Virtualized lists (cache rendered DOM nodes).
  * CDN/browser caching uses LRU under the hood.

* **Performance Tradeoffs**:

  * Map-based version: simpler, usually fine for small caches (<1000).
  * Linked list version: O(1) guaranteed, better for large caches.
  * Memory overhead: extra linked list nodes.
  * For huge caches: consider LFU (Least Frequently Used) instead of LRU.

---

# ðŸŽ¯ Final Interview Takeaways (LRU Cache)

* âœ… Step 1: FIFO naive.
* âœ… Step 2: Map-based LRU.
* âœ… Step 3: HashMap + DLL for O(1).
* âœ… Step 4: Add delete & clear.
* âœ… Step 5: Discuss frontend usage + tradeoffs.